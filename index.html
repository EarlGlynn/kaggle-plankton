<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1,
      maximum-scale=1">
    <link
      href="https://fonts.googleapis.com/css?family=Architects+Daughter"
      rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css"
      href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css"
      href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css"
      media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Kaggle Plankton Project by EarlGlynn</title>
  </head>
  <body>
    <header>
      <div class="inner">
        <h1>Kaggle Plankton Project</h1>
        <h2>Predicting plankton classes from images</h2>
        <a href="https://github.com/EarlGlynn/kaggle-plankton"
          class="button"><small>View project on</small> GitHub</a> </div>
    </header>
    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content"> Earl F Glynn<br>
          <br>
          Work in progress:&nbsp; updated 2015-02-15<br>
          <h3> <a id="intro" class="anchor" href="#intro"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction













            / Overview<br>
          </h3>
          <p>These are rough notes outlining the process used to create
            plankton classifiers for the Kaggle National Data Science
            Bowl competition, <a
              href="https://www.kaggle.com/c/datasciencebowl"><i>Predict
                ocean health, one plankton at a time</i></a>.&nbsp;
            These notes are an attempt to have a defined and repeatable
            process.<br>
          </p>
          <p>In brief, the problem is to take 30,336 images from 121
            known plankton classes, and create classifier(s) to predict
            the class of 130,400 test images. <br>
          </p>
          <p> </p>
          <p><b>Sample plankton training images from the class
              "acantharia_protist_big_center"</b><br>
          </p>
          <img alt="Sample Plankton Training Images" title="Sample
            Plankton Training Images"
            src="images/Sample-Plankton-Train-Images.jpg" height="240"
            width="454"><br>
          <br>
          <b>Sample plankton test images from unknown classes</b><br>
          <img alt="Sample Plankton test images" title="Sample Plankton
            test images" src="images/Sample-Plankton-Test-Images.JPG"
            height="218" width="455"><br>
          <h3> <a id="tutorials" class="anchor" href="#tutorials"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>Tutorials













            [Windows]<br>
          </h3>
          <p>I went through two online tutorials to understand the data
            and possible processing a bit better: </p>
          <ul>
            <li>Slightly modified version of original tutorial by Aaron
              Sander: IPython notebook <a
                href="Sander-Kaggle-Plankton-Tutorial.html">Sander-Kaggle-Plankton-Tutorial.html</a>
            </li>
            <li>Slightly modified version of tutorial by Ehud
              Ben-Reuven:<span style="mso-spacerun:yes">&nbsp; </span>IPython













              notebook <a
                href="BenReuven-Kaggle-Plankton-tutorial-submission.html">BenReuven-Kaggle-Plankton-tutorial-submission.html</a></li>
          </ul>
          Note:&nbsp; HTML versions of IPython notebooks are linked
          here, but the operational IPython notebooks are in the
          repository. <br>
          <ul>
          </ul>
          <p> </p>
          <h3> <a id="explorations" class="anchor" href="#explorations"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>Explorations













            [Windows]</h3>
          <p>Additional experiments:<br>
          </p>
          <ul>
            <li>Experiments showing more details than original tutorial:
              IPython notebook <a href="Plankton%20Explorer.html">Plankton






                Explorer.html</a></li>
            <li>Explore Python’s <i>skimage regionprops</i> for image
              features: IPython notebook <a
                href="Plankton%20skimage%20region%20properties.html">Plankton












                skimage region properties.html </a></li>
          </ul>
          <ul>
          </ul>
          <h3> <a id="duplicates" class="anchor" href="#duplicates"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>Duplicate













            Images [Windows]</h3>
          <p>Many duplicates images, which were mostly in the test set,
            were found by computing and comparing <a
              href="http://en.wikipedia.org/wiki/Md5sum">md5sums</a>.&nbsp;









            <br>
          </p>
          <p>See example duplicates:<br>
          </p>
          <ul>
            <li><b>Train</b>:&nbsp; <a href="01-train-duplicates.pdf">01-train-duplicates.pdf</a>.&nbsp;









              Train images 14864.jpg and 271.jpg, oddly assigned to two
              different classes, were found to be duplicates using
              script <b>01-train-md5.R</b>.<br>
            </li>
            <li><b>Test</b>:&nbsp; <a href="04-test-duplicates.pdf">04-test-duplicates.pdf</a>.&nbsp;










              Script <b>04-test-md5.R</b> proved there were only 93,502
              unique images in the set of 130,400.&nbsp; That means
              36,898 of the image are duplicates!&nbsp; <b>File
                04-Plankton-Test-FileList-Duplicates.csv</b> in the
              repository shows all the duplicate files.<br>
            </li>
            <li><b>Train-Test</b>:&nbsp; <a
                href="04-test-train-duplicate.pdf">
                04-test-train-duplicate.pdf</a>.&nbsp; The link shows
              the two pairs of duplicates found between the test and
              train sets.<br>
            </li>
          </ul>
          <i>Posted to Kaggle forum</i>:<br>
          <p><i>… after getting the complete set of 130,400 test images
              from a new copy of test.zip, the duplicate count in the
              test set is now 36,898 -- there are only 93,502 unique
              test set images. The counts of the number of kinds of
              duplicates is now:</i></p>
          <pre><code>[1] 93502
counts
    1     2     3     4     5     6     7     8     9
71244 12346  6393  2597   684   195    38     4     1    
</code></pre>
          <p><i>So, 71,244 test images are completely unique. At the
              other extreme, there is one case of 9 images being
              identically the same. There are four cases of 8 images
              being identically the same. <br>
            </i></p>
          <p><i>I would have thought minor variations in the test set
              (e.g, a variety of rotations of a particular image) would
              be a better test strategy than just repeating the same
              images so many times. . . .<br>
            </i></p>
          <p><b>Example duplicate test images</b><i><br>
              <img alt="Example Duplicate Test Images" title="Example
                Duplicate Test Images"
                src="images/Example-Duplicate-Test-Images.jpg"
                height="261" width="371"><br>
            </i></p>
          <h3> <a id="wndchrm" class="anchor" href="#wndchrm-features"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>
            wndchrm Image Features<br>
          </h3>
          <p>Instead of pursuing <i>regionprops</i> for a relatively
            small number of additional image properties, a program <i>wndchrm</i>
            was found that computes almost 3,000 features for each
            image.<br>
          </p>
          <p>The <b>wndchrm command-line utility</b> is described in
            these two papers:</p>
          <ul>
            <li>Shamir L, Orlov N, Eckley DM, Macura T, Johnston J,
              Goldberg IG. <b>Wndchrm - an open source utility for
                biological image analysis</b>. <i>BMC Source Code for
                Biology and Medicine.</i> 3: 13, 2008. [<a
                href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2478650/">PubMed</a>].</li>
            <li>Orlov N, Shamir L, Macura T, Johnston J, Eckley DM,
              Goldberg IG. <b>WND-CHARM: Multi-purpose image
                classification using compound image transform</b><b>s</b>.
              <i>Pattern Recognition Letters</i>. 29(11): 1684-93,
              2008.&nbsp; [<a
                href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2573471/">PubMed</a>].</li>
          </ul>
          <p>The current source code can be found on GitHub:&nbsp; <a
              href="https://github.com/wnd-charm/wnd-charm">wnd-charm</a>,&nbsp;




            <i>A generalized pattern recognition system for images
              developed by the Goldberg group at the NIH/NIA. </i><br>
          </p>
          <p><a
              href="http://vfacstaff.ltu.edu/lshamir/downloads/ImageClassifier/">Last





              known Windows wndchrm.exe</a>.<br>
          </p>
          <p>Since the latest <i>wndchrm</i> utility is available more
            easily under Linux, that version was used, which required
            converting all .jpg files to .tifs.<br>
          </p>
          <h3> <a id="conversions" class="anchor" href="#conversions"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>
            Image Format Conversion [CentOS virtual machine]<br>
          </h3>
          <meta http-equiv="Content-Type" content="text/html;
            charset=UTF-8">
          <p><i>wndchrm</i> requires .tif images as input, so <a
              href="http://www.imagemagick.org/">ImageMagick</a> was
            used to convert all the original .jpg images to .tifs.</p>
          <ul>
            <li>Convert train and test images from jpgs to tifs for use
              with wndchrm:&nbsp; IPython notebook <a
                href="Plankton-Images-Convert-jpgs-to-tifs.html">Plankton-Images-Convert-jpgs-to-tifs.html</a>&nbsp;





              [Train numbers for each of 121 directories – oops
              directory count was 122. Test total count is 130,400.]</li>
            <li>Fix problems with two directories by re-running
              conversion for those directories after manually cleaning
              some directory problems:&nbsp; <a
                href="Convert%20two%20notebooks%20jpg%20to%20tif.html">Convert










                two notebooks jpg to tif.html</a></li>
            <li>Count files by train directory to verify correct
              numbers:&nbsp; <a href="Count-Train-Files.html">Count-Train-Files.html</a>
              [Train total is 30,336. Corrected train counts by
              directory: <a href="Plankton-Train-Classes.pdf">Plankton-Train-Classes.pdf</a>.]</li>
          </ul>
          <h3> <a id="wndchrm" class="anchor" href="#wndchrm"
              aria-hidden="true"><span class="octicon octicon-link"></span></a>Computation












            of Image wndchrm Features [CentOS VM]<br>
          </h3>
          <ul>
            <li><i>wndchrm</i> accepts tifs but not jpgs, which is why
              the step above was to convert images.<br>
            </li>
            <li><b>WndCharm Notes</b> (25 pages): <a
                href="WndCharm-Notes.pdf">WndCharm-Notes.pdf</a> gives
              details of building Linux version of <i>wndchrm</i>, and
              creating .sig image feature files for all training and
              test images.</li>
            <li>View text file <a href="100224-l-sig.txt">100224-l.sig</a>
              with computed features for image <b>100224.tif</b>.<br>
            </li>
            <li>Unfortunately, output HTML file from <i>wndchrm</i> for
              processing training images is about 300 MB, which is too
              big to be included here or on GitHub.</li>
          </ul>
          <h3> <a id="featurematrix" class="anchor"
              href="#featurematrix" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>Create R wndchrm
            Train and Test Feature Matrices [Windows]<br>
          </h3>
          <ul>
            <li>Combine all <i>wndchrm</i> .sig output files into
              single feature matrix for machine learning experiments for
              the training images (<b>wndchrm-train-data.Rmd</b>).&nbsp;
              The file <b>plankton-train-wndchrm-features.Rdata</b> is
              258 MB in size.&nbsp; See <a
                href="wndchrm-train-data.html">wndchrm-train-data.html</a></li>
            <li>The features associated with the largest 75 eigenvalues
              from SVD analysis of the training data explain about 65%
              of variance.&nbsp; See <a
                href="wndchrm-train-svd-pca.html">wndchrm-train-svd-pca.html</a>.&nbsp;


              The first three principal components of the training
              feature matrix can be viewed using the R pca3d package:<br>
            </li>
          </ul>
          <p><b>First three principal components of training feature
              matrix</b><br>
            <img alt="First 3 Principal Components of Training Feature
              Matrix" title="First 3 Principal Components of Training
              Feature Matrix"
              src="images/plankton-wndchrm-features-PCA-view1.png"
              height="679" width="656"><br>
          </p>
          <ul>
          </ul>
          This plot shows the 121 plankton classes by various colors,
          but not all classes are given unique colors here.<br>
          <br>
          The file <b>plankton-train-wndchrm-svd-pca.Rdata</b>, which
          contains computed SVD and PCA results, is nearly 1.7 GB in
          size.<br>
          <ul>
            <li>Create a similar feature matrix for the test images and
              save to R .Rdata file (<b>wndchrm-test-data.Rmd</b>).&nbsp;




              The file <b>plankton-test-wndchrm-features.Rdata</b> is
              1.1 GB in size. See <a href="wndchrm-test-data.html">wndchrm-test-data.htm</a>l.&nbsp;</li>
          </ul>
          <h3> <a id="caret-experiments" class="anchor"
              href="#caret-experiments" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>Caret Machine
            Learning Experiments [Windows/Linux]<br>
          </h3>
          <p>to be continued ...<br>
          </p>
          <br>
        </section>
        <aside id="sidebar"> <a
            href="https://github.com/EarlGlynn/kaggle-plankton/zipball/master"
            class="button"> <small>Download</small> .zip file </a> <a
href="https://github.com/EarlGlynn/kaggle-plankton/tarball/master"
            class="button"> <small>Download</small> .tar.gz file </a>
          <p class="repo-owner"> is maintained by <a
              href="https://github.com/EarlGlynn">EarlGlynn</a>.</p>
          <p>This page was generated by <a
              href="https://pages.github.com">GitHub Pages</a> using the
            Architect theme by <a href="https://twitter.com/jasonlong">Jason

















              Long</a>.</p>
        </aside>
      </div>
    </div>
  </body>
</html>
